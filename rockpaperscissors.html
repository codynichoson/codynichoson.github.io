<!DOCTYPE HTML>
<!--
	Massively by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Cody Nichoson - Rock, Paper, Scissors</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
        <noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>

	</head>
	<body class="is-preload">

		<div id="wrapper" >

				<!-- Header -->
					<header id="header">
                        <a href="index.html" class="logo">Cody Nichoson</a>
					</header>

				<!-- Nav -->
					<nav id="nav">
						<ul class="links">
							<li class="active"><a href="index.html">Projects</a></li>
							<li> <a href="about.html">About</a></li>
							<li><a href="resume.html">Resume</a></li>
							<!-- <li><a href="contact.html">Contact</a></li> -->
						</ul>
						<ul class="icons">
							<li><a href="https://www.linkedin.com/in/codynichoson/" class="icon brands fa-linkedin"><span class="label">LinkedIn</span></a></li>
							<li><a href="https://github.com/codynichoson" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">

						<!-- Post -->
							<section class="post">
								<header class="major">
									<!-- <span class="date">April 2021 - December 2021</span> -->
									<h1>Rock, Paper, Scissors Robot</h1>
                                    <p>Created a rock, paper, scissors partner using an Allegro Robotic Hand and computer vision.
									</p>
                                </header>

								<div class="iframe-container">
									<iframe width="853" height="480" src="https://www.youtube.com/embed/roJrrsgUx8o" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
								</div>
								<style>
									.iframe-container {
										  text-align:center;
											width:100%;
									}
								</style><br/>

								<h2>Overview</h2>
								<p> In this solo project, I set out to create a rock, paper, scissors robotic partner that would allow
									a user to play in a similar fashion as they would another human. Meeting this requirement meant that
									some creative computer vision techniques were necessary to facilitate gameplay using natural gestures.
									The project made use of the Wonik Allegro Robotic Hand, OpenCV computer vision library, and MediaPipe
									machine learning library in order to create the final result.
                                </p>

								<h2>System Breakdown</h2>
								<p> In order to complete this project, it was divided into two nodes:<br><br>

                                    1. Perception - Determine location of the ball relative to the camera<br>
                                    2. Motion Control - Control the robot to move to the desired location<br>
                                </p>

                                <h2>Perception</h2>
								<p> The primary mechanism of playing rock, paper, scissors is proper recognition of the game's
									associated gestures. To accomplish this, I utilized the MediaPipe machine learning library 
									and its built-in hand recognition API. The MediaPipe library is able to take inputted video frames,
									identify a human hand, and attach landmark coordinates to pre-defined parts of the hand. I then used
									relationships between these landmarks to define gestures that would eventually become rock, paper, 
									and scissors.</br><br>

									For example, the "scissors" gesture was defined as the pinky and ring fingertips being within a certain
									small distance threshold of their respective knuckle landmarks, the index and middle fingertips being within a 
									certain larger distance threshold of their respective knuckle landmarks, and the index and middle fingers
									forming an acute angle. Of course, these various thresholds required some tuning to result in consistent
									results with different hands and distances from the camera.
                                </p>

								<center>
									<div class="image main"><img src="images/mediapipe_gestures.gif" alt="" style="max-width: 70%"/></div>
								</center>

								<p> After the gestures were defined, starting criteria for the game were needed. In a typical game of
									rock, paper, scissors, the game is usually started by the players moving their fists up and down in
									a countdown to "shoot!" In a first iteration, this countdown was done by simply iterating a counter
									everytime the landmark at the base of the palm crossed a specified row in frame. This approach is not
									very robust or intuitive to use, so a better method was needed.</br><br>

									To improve the countdown, the velocity of the hand was calculated to keep track of the direction and
									magnitude that it's moving in frame. When the user's hand velocity displays a pattern of positive and
									negative velocities in the y-axis that corresponds to a typical countdown pattern, the counter is then
									incremented in a similar fashion to the previous iteration.</br><br>

									Once the counter is incremented enough and the game is started, the perception node then publishes the
									next detected gesture from the user. It is this published gesture that lets the robot know that the 
									game has started and that it should pick a gesture.
                                </p>

								<h2>Motion Control</h2>
								<p> Once the motion control node receives a human gesture from the perception node, it knows that the game
									has officially started and it must picked a gesture. The robot hand gestures for rock, paper, and scissors
									are defined as joint state vectors and published to the Allegro hand through the use of the BHand library.
									This library takes in a list of joint states for each motor on the hand and runs a control system to maintain
									them.</br><br>

									At the moment, the gestures that the hand chooses to play are randomly chosen. This results in a rock, paper,
									scissors experience that is very similar to how two humans would play. In future iterations, prediction
									techniques could be implemented to guess the human's gesture choice immediately before it chooses its own
									pose in an attempt to "cheat" and always pick the winning gesture.
                                </p>	
								
								<h1 style="text-align:center"><a href="https://github.com/codynichoson/RockPaperScissorsRobot" target="_blank">View on GitHub!</a></h1>

							</section>

					</div>

				<!-- Copyright -->
					<div id="copyright">
						<ul><li>&copy; Cody Nichoson</li><li>Design: <a href="https://html5up.net">HTML5 UP</a></li></ul>
					</div>

		</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>